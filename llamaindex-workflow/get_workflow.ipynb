{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook only seeks to draw the workflow out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Annotated, List, Any\n",
    "\n",
    "class SearchEvent(Event):\n",
    "    \"\"\"Requires the LLM to do an online search to answer the question\"\"\"\n",
    "    query: Annotated[str, \"The user's query\"]\n",
    "\n",
    "class AnswerEvent(Event):\n",
    "    \"\"\"Allows the LLM to answer the question without searching\"\"\"\n",
    "    query: Annotated[str, \"The user's query\"]\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    \"\"\"Collects LLM response\"\"\"\n",
    "    query: Annotated[str, \"The user's query\"]\n",
    "    answer: Annotated[str, \"The LLM's response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "### Define tools\n",
    "search_tool_spec = TavilyToolSpec(api_key=os.getenv(\"TAVILY\"))\n",
    "search_tools = search_tool_spec.to_tool_list()\n",
    "\n",
    "### Define events\n",
    "class SearchEvent(Event):\n",
    "    \"\"\"Requires the LLM to do an online search to answer the question\"\"\"\n",
    "    query: Annotated[str, \"The user's query\"]\n",
    "\n",
    "class AnswerEvent(Event):\n",
    "    \"\"\"Allows the LLM to answer the question without searching\"\"\"\n",
    "    query: Annotated[str, \"The user's query\"]\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    \"\"\"Collects LLM response\"\"\"\n",
    "    query: Annotated[str, \"The user's query\"]\n",
    "    answer: Annotated[str, \"The LLM's response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfAnswers(Workflow):\n",
    "    def __init__(\n",
    "        self, \n",
    "        *args: Any,\n",
    "        llm: Optional[LLM] = llm,\n",
    "        **kwargs: Any\n",
    "    ):\n",
    "        \"\"\"Class constructor. Takes in an llm instance and constructs \n",
    "        1. A function calling agent with search tools\n",
    "        2. A simple chat engine instance\n",
    "        3. A common memory instance across the workflow\n",
    "\n",
    "        Args:\n",
    "            llm (Optional[LLM], optional): LLM instance. Defaults to Settings.llm.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.llm = llm\n",
    "        self.search_agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "            tools = search_tools,\n",
    "            llm = self.llm\n",
    "        )\n",
    "        self.search_agent = self.search_agent_worker.as_agent()\n",
    "        self.answer_without_search_engine = SimpleChatEngine.from_defaults(\n",
    "            llm = self.llm\n",
    "        )\n",
    "        self.history: List[ChatMessage] = []\n",
    "    \n",
    "    @step()\n",
    "    async def route_to_llm(\n",
    "        self,\n",
    "        ev: StartEvent\n",
    "    ) -> SearchEvent | AnswerEvent:\n",
    "        \"\"\"Generates a search event and an answer event once given a start event\"\"\"\n",
    "        \n",
    "        ## Update memory\n",
    "        self.history.append(\n",
    "            ChatMessage(\n",
    "                role = MessageRole.USER,\n",
    "                content = ev.query\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        ## Routes to both events. But you can also write a router component to decide \n",
    "        ## which event to route to.\n",
    "        self.send_event(SearchEvent(query = ev.query))\n",
    "        self.send_event(AnswerEvent(query = ev.query))\n",
    "    \n",
    "    @step()\n",
    "    async def search_and_answer(\n",
    "        self,\n",
    "        ev: SearchEvent\n",
    "    ) -> ResponseEvent:\n",
    "        \"\"\"Uses the tavily search tool to answer the question\"\"\"\n",
    "        \n",
    "        ## Synthesize response\n",
    "        response = await self.search_agent.achat(\n",
    "            ev.query, \n",
    "            chat_history = self.history\n",
    "        )\n",
    "        \n",
    "        ## [OPTIONAL] Show intermediate response in the frontend\n",
    "        # await cl.Message(content=\"ANSWER WITH SEARCH: \" + str(response)).send()\n",
    "        \n",
    "        ## Update memory\n",
    "        self.history.append(\n",
    "            ChatMessage(\n",
    "                role = MessageRole.ASSISTANT,\n",
    "                content = \"ANSWER WITH SEARCH: \" + str(response)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return ResponseEvent(query = ev.query, answer = str(response))\n",
    "\n",
    "    @step()\n",
    "    async def simply_answer(\n",
    "        self,\n",
    "        ev: AnswerEvent\n",
    "    ) -> ResponseEvent:\n",
    "        \"\"\"Uses the LLM to simple answer the question\"\"\"\n",
    "        \n",
    "        ## Synthesize response\n",
    "        response = await self.answer_without_search_engine.achat(\n",
    "            ev.query, \n",
    "            chat_history = self.history\n",
    "        )\n",
    "        \n",
    "        ## [OPTIONAL] Show intermediate response in the frontend\n",
    "        # await cl.Message(content=\"ANSWER WITHOUT SEARCH: \" + str(response)).send()\n",
    "        \n",
    "        ## Update memory\n",
    "        self.history.append(\n",
    "            ChatMessage(\n",
    "                role = MessageRole.ASSISTANT,\n",
    "                content = \"ANSWER WITHOUT SEARCH: \" + str(response)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return ResponseEvent(query = ev.query, answer = str(response))\n",
    "    \n",
    "    @step()\n",
    "    async def compile(\n",
    "        self,\n",
    "        ctx: Context,\n",
    "        ev: ResponseEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Compiles and summarizes answers from all response events\"\"\"\n",
    "        \n",
    "        ## There are 2 response events from routing to 2 different agents. This can\n",
    "        ## also be a dynamic number of events.\n",
    "        ready = ctx.collect_events(ev, [ResponseEvent] * 2) \n",
    "        \n",
    "        if ready is None:\n",
    "            return None\n",
    "        \n",
    "        response = await self.llm.acomplete(\n",
    "            f\"\"\"\n",
    "            A user has asked us a question and we have responded accordingly using a \n",
    "            search tool and without using a search tool. Your job is to decide which \n",
    "            response best answered the question and summarize the response into a crisp \n",
    "            reply. If both responses answered the question, summarize both responses\n",
    "            into a single answer.\n",
    "            \n",
    "            The user's query was: {ev.query}\n",
    "            \n",
    "            The responses are:\n",
    "            {ready[0].answer} &\n",
    "            {ready[1].answer}\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        ## Update memory\n",
    "        self.history.append(\n",
    "            ChatMessage(\n",
    "                role = MessageRole.ASSISTANT,\n",
    "                content = \"FINAL ANSWER: \" + str(response)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return StopEvent(result = str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(MixtureOfAnswers, filename=\"workflow.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fb762c9777d81b920bdc4f2aaae00fd18dcefdd6f60d8272d61e705ce6e5d33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
